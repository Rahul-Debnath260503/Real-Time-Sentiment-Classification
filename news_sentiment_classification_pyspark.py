# -*- coding: utf-8 -*-
"""News_Sentiment_Classification_PySpark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkOeuvthdhqGs49S2bgUV68rf3zysGpN
"""

# Step 1: Fetch News Headlines from GNews API

import requests  # Library to make HTTP requests

# 1. Set your GNews API key
api_key = "f923b3c1b261dba46561aa6c95a5bc54"  # Replace with your actual API key

# 2. GNews API endpoint for top headlines in English
url = f"https://gnews.io/api/v4/top-headlines?token={api_key}&lang=en"

# 3. Make a GET request to the API
response = requests.get(url)

# 4. Convert the response to JSON format
data = response.json()

# 5. Extract headlines from the articles
headlines = [article['title'] for article in data['articles']]

# 6. Print the headlines
print("Top Headlines from GNews:")
for i, h in enumerate(headlines, start=1):
    print(f"{i}. {h}")

# Step 2: Convert headlines into a PySpark DataFrame

from pyspark.sql import SparkSession

# 1. Create a Spark session
spark = SparkSession.builder.appName("NewsSentiment").getOrCreate()

# 2. Convert the Python list of headlines into a list of tuples
headline_tuples = [(h,) for h in headlines]  # Each headline needs to be a tuple

# 3. Create a PySpark DataFrame with column name "headline"
df_headlines = spark.createDataFrame(headline_tuples, ["headline"])

# 4. Show the DataFrame
df_headlines.show(truncate=False)

# Step 3: Assign a label column based on which class has the maximum score

from pyspark.sql.functions import greatest, when, col

# Load your Kaggle CSV dataset
df_csv = spark.read.csv("/content/Times_of_India_Healines_since_jan_2020_score.csv", header=True, inferSchema=True)

df_csv.show()

# Create 'label' column:
# 0 = Negative, 1 = Positive, 2 = Neutral, 3 = Compound
df_labeled = df_csv.withColumn(
    "label",
    when(col("Positive").cast("double") == greatest(col("Positive").cast("double"), col("Negative").cast("double"), col("Neutral"), col("Compound")), 1)
    .when(col("Negative").cast("double") == greatest(col("Positive").cast("double"), col("Negative").cast("double"), col("Neutral"), col("Compound")), 0)
    .when(col("Neutral") == greatest(col("Positive").cast("double"), col("Negative").cast("double"), col("Neutral"), col("Compound")), 2)
    .otherwise(3)  # Compound
)

# Keep only the headline text and the new label
df_train = df_labeled.select(col("Headline").alias("headline"), "label")

# Show sample
df_train.show(10, truncate=False)

df_train.filter(df_train['label'] == 0).show(truncate=False)

# Step 4: Build and Train PySpark ML Pipeline

from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

# 1. Define the stages of the pipeline
tokenizer = Tokenizer(inputCol="headline", outputCol="words")          # Split headline into words
remover = StopWordsRemover(inputCol="words", outputCol="filtered")    # Remove common stopwords
hashingTF = HashingTF(inputCol="filtered", outputCol="features")      # Convert words into numeric features
lr = LogisticRegression(labelCol="label", featuresCol="features", maxIter=20, family="multinomial")  # Multi-class LR

# 2. Create the pipeline
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, lr])

# 3. Train the model on the labeled dataset
model = pipeline.fit(df_train)

print("Multi-class Sentiment Classification Model Trained Successfully!")

# Step 5: Classify live GNews headlines

# Use the trained model to predict the sentiment of new headlines
predictions = model.transform(df_headlines)

# Show the headlines with predicted labels
# 0 = Negative, 1 = Positive, 2 = Neutral, 3 = Compound
predictions.select("headline", "prediction").show(truncate=False)

# Save the trained PySpark pipeline model
model.write().overwrite().save("sentiment_model")

!zip -r sentiment_model.zip sentiment_model

